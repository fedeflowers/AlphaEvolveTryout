{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfgcnVUJ/cO1fn2GyoN7LG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedeflowers/AlphaEvolveTryout/blob/main/Autolavaggio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6Ohz9BGigFM",
        "outputId": "2207d1b2-6a6a-4fba-91f3-13fb81865ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50daa68c",
        "outputId": "e697db61-b11f-4b93-f13b-3ba554b8cc02"
      },
      "source": [
        "import os\n",
        "\n",
        "root_drive_path = '/content/drive/MyDrive'\n",
        "VERSION = \"v3\"\n",
        "\n",
        "\n",
        "# Define the relative paths provided by the user\n",
        "path1_relative = 'tex_cars/CatalogatedImagesAI/GenerateAI/pista1/OpenHood'\n",
        "path2_relative = 'tex_cars/CatalogatedImagesAI/GenerateAI/pista1/Original'\n",
        "base_model_dir = os.path.join(root_drive_path, 'tex_cars')\n",
        "\n",
        "mobilenet_save_dir = os.path.join(base_model_dir, 'mobilenet', VERSION)\n",
        "yolo_save_dir = os.path.join(base_model_dir, 'yolo', VERSION)\n",
        "\n",
        "os.makedirs(mobilenet_save_dir, exist_ok=True)\n",
        "os.makedirs(yolo_save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Models will be saved to:\\n - {mobilenet_save_dir}\\n - {yolo_save_dir}\")\n",
        "\n",
        "\n",
        "# Construct full paths\n",
        "full_path1 = os.path.join(root_drive_path, path1_relative)\n",
        "full_path2 = os.path.join(root_drive_path, path2_relative)\n",
        "\n",
        "image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff')\n",
        "\n",
        "def list_image_files(directory_path):\n",
        "    images = []\n",
        "    if os.path.exists(directory_path):\n",
        "        print(f\"Listing images in: {directory_path}\")\n",
        "        for filename in os.listdir(directory_path):\n",
        "            if filename.lower().endswith(image_extensions):\n",
        "                images.append(os.path.join(directory_path, filename))\n",
        "    else:\n",
        "        print(f\"Directory not found: {directory_path}\")\n",
        "    return images\n",
        "\n",
        "print(\"\\n--- Images from OpenHood ---\")\n",
        "open_hood_images = list_image_files(full_path1)\n",
        "# for img in open_hood_images:\n",
        "#     print(img)\n",
        "\n",
        "print(\"\\n--- Images from Original ---\")\n",
        "original_images = list_image_files(full_path2)\n",
        "# for img in original_images:\n",
        "#     print(img)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models will be saved to:\n",
            " - /content/drive/MyDrive/tex_cars/mobilenet/v3\n",
            " - /content/drive/MyDrive/tex_cars/yolo/v3\n",
            "\n",
            "--- Images from OpenHood ---\n",
            "Listing images in: /content/drive/MyDrive/tex_cars/CatalogatedImagesAI/GenerateAI/pista1/OpenHood\n",
            "\n",
            "--- Images from Original ---\n",
            "Listing images in: /content/drive/MyDrive/tex_cars/CatalogatedImagesAI/GenerateAI/pista1/Original\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(original_images), len(open_hood_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrcx7xJAo0Z3",
        "outputId": "4639bced-4611-40eb-e06b-d90999fa0927"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(306, 306)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Get list of filenames (assuming filenames are identical in both folders)\n",
        "# We use the 'Original' folder as the source of truth for unique cars\n",
        "original_filenames = [os.path.basename(x) for x in original_images]\n",
        "\n",
        "# 2. Split the FILENAMES, not the images\n",
        "train_names, val_names = train_test_split(original_filenames, test_size=0.2, random_state=42)\n",
        "\n",
        "def build_dataset(filenames, original_paths, open_paths):\n",
        "    data = []\n",
        "    # Create a lookup for open_paths for speed\n",
        "    open_map = {os.path.basename(p): p for p in open_paths}\n",
        "    original_map = {os.path.basename(p): p for p in original_paths}\n",
        "\n",
        "    for name in filenames:\n",
        "        # Add the 'Closed' version (Label 0)\n",
        "        if name in original_map:\n",
        "            data.append({\n",
        "                'filepath': original_map[name],\n",
        "                'is_hood_open': 0,\n",
        "                'car_id': name # Useful for debugging\n",
        "            })\n",
        "\n",
        "        # Add the 'Open' version (Label 1)\n",
        "        # Only add if it exists in the Open folder\n",
        "        if name in open_map:\n",
        "            data.append({\n",
        "                'filepath': open_map[name],\n",
        "                'is_hood_open': 1,\n",
        "                'car_id': name\n",
        "            })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# 3. Build the DataFrames\n",
        "train_df = build_dataset(train_names, original_images, open_hood_images)\n",
        "val_df = build_dataset(val_names, original_images, open_hood_images)\n",
        "\n",
        "# Convert for Keras\n",
        "train_df['is_hood_open_str'] = train_df['is_hood_open'].astype(str)\n",
        "val_df['is_hood_open_str'] = val_df['is_hood_open'].astype(str)\n",
        "\n",
        "print(f\"Training Set: {len(train_df)} images\")\n",
        "print(f\"Validation Set: {len(val_df)} images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuprLo4GnJSb",
        "outputId": "a783e510-468f-4e57-d6dc-ff1d5d4e2364"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set: 488 images\n",
            "Validation Set: 124 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_df.head(10)['filepath'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dMI7afLcY9wk",
        "outputId": "fc230c05-e22a-4b35-d3e5-4e5d36fbf01e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/tex_cars/CatalogatedImagesAI/GenerateAI/pista1/Original/pista1_091125_135823.png'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras import layers, models\n",
        "# import os\n",
        "# from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "\n",
        "# # --- A. Preprocessing & Loading ---\n",
        "# BATCH_SIZE = 32\n",
        "# IMG_SIZE = (224, 224)\n",
        "\n",
        "# train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "#     preprocessing_function=preprocess_input,\n",
        "#     rotation_range=20\n",
        "# )\n",
        "# val_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "#     preprocessing_function=preprocess_input\n",
        "# )\n",
        "\n",
        "# train_data = train_gen.flow_from_dataframe(\n",
        "#     train_df, x_col='filepath', y_col='is_hood_open_str',\n",
        "#     target_size=IMG_SIZE, class_mode='binary', batch_size=BATCH_SIZE\n",
        "# )\n",
        "# val_data = val_gen.flow_from_dataframe(\n",
        "#     val_df, x_col='filepath', y_col='is_hood_open_str',\n",
        "#     target_size=IMG_SIZE, class_mode='binary', batch_size=BATCH_SIZE\n",
        "# )\n",
        "\n",
        "# # --- B. Model Definition ---\n",
        "# base_model = tf.keras.applications.MobileNetV3Small(\n",
        "#     input_shape=IMG_SIZE + (3,), include_top=False, weights='imagenet'\n",
        "# )\n",
        "# base_model.trainable = False\n",
        "\n",
        "# model = models.Sequential([\n",
        "#     base_model,\n",
        "#     layers.GlobalAveragePooling2D(),\n",
        "#     layers.Dropout(0.2),\n",
        "#     layers.Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy', tf.keras.metrics.Recall(name='recall')])\n",
        "\n",
        "# # --- C. Training ---\n",
        "# # Class 0 = Closed (Normal), Class 1 = Open (Fault)\n",
        "# # Weight 7.0 on Class 1 forces the model to prioritize detecting Open hoods (Recall)\n",
        "# class_weights = {0: 1.0, 1: 1.0}\n",
        "# model.fit(train_data, validation_data=val_data, epochs=100, class_weight=class_weights)\n",
        "\n",
        "# # --- D. Export to TFLite ---\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# mn_path = os.path.join(mobilenet_save_dir, 'hood_classifier.tflite')\n",
        "# with open(mn_path, 'wb') as f:\n",
        "#     f.write(tflite_model)\n",
        "# print(f\"MobileNet Saved: {mn_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUAhlgoRnKeU",
        "outputId": "239d3480-7020-407a-bc5d-134130ab38e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 488 validated image filenames belonging to 2 classes.\n",
            "Found 124 validated image filenames belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.5377 - loss: 0.7925 - recall: 0.3572 - val_accuracy: 0.5726 - val_loss: 0.6636 - val_recall: 0.6129\n",
            "Epoch 2/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5330 - loss: 0.7451 - recall: 0.4138 - val_accuracy: 0.6129 - val_loss: 0.6638 - val_recall: 0.8387\n",
            "Epoch 3/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6123 - loss: 0.6763 - recall: 0.5649 - val_accuracy: 0.6048 - val_loss: 0.6603 - val_recall: 0.9032\n",
            "Epoch 4/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5623 - loss: 0.6959 - recall: 0.5554 - val_accuracy: 0.6452 - val_loss: 0.6453 - val_recall: 0.9032\n",
            "Epoch 5/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.5667 - loss: 0.7191 - recall: 0.5309 - val_accuracy: 0.6532 - val_loss: 0.6391 - val_recall: 0.9355\n",
            "Epoch 6/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5972 - loss: 0.6776 - recall: 0.6004 - val_accuracy: 0.6613 - val_loss: 0.6239 - val_recall: 0.9194\n",
            "Epoch 7/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6257 - loss: 0.6687 - recall: 0.6066 - val_accuracy: 0.6855 - val_loss: 0.6100 - val_recall: 0.9032\n",
            "Epoch 8/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.5879 - loss: 0.6680 - recall: 0.5913 - val_accuracy: 0.7097 - val_loss: 0.6005 - val_recall: 0.9355\n",
            "Epoch 9/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6347 - loss: 0.6413 - recall: 0.6246 - val_accuracy: 0.7016 - val_loss: 0.5964 - val_recall: 0.9516\n",
            "Epoch 10/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6195 - loss: 0.6616 - recall: 0.6661 - val_accuracy: 0.7177 - val_loss: 0.5832 - val_recall: 0.9516\n",
            "Epoch 11/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.6328 - loss: 0.6255 - recall: 0.6713 - val_accuracy: 0.7339 - val_loss: 0.5698 - val_recall: 0.9355\n",
            "Epoch 12/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6769 - loss: 0.5933 - recall: 0.6900 - val_accuracy: 0.7581 - val_loss: 0.5545 - val_recall: 0.9355\n",
            "Epoch 13/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6928 - loss: 0.5920 - recall: 0.6722 - val_accuracy: 0.7500 - val_loss: 0.5473 - val_recall: 0.9355\n",
            "Epoch 14/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6927 - loss: 0.5685 - recall: 0.6753 - val_accuracy: 0.7661 - val_loss: 0.5407 - val_recall: 0.9677\n",
            "Epoch 15/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6800 - loss: 0.5840 - recall: 0.6247 - val_accuracy: 0.7984 - val_loss: 0.5304 - val_recall: 0.9677\n",
            "Epoch 16/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7178 - loss: 0.5724 - recall: 0.7188 - val_accuracy: 0.8065 - val_loss: 0.5215 - val_recall: 0.9677\n",
            "Epoch 17/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.6728 - loss: 0.5882 - recall: 0.6488 - val_accuracy: 0.8145 - val_loss: 0.5153 - val_recall: 0.9839\n",
            "Epoch 18/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7109 - loss: 0.5731 - recall: 0.7039 - val_accuracy: 0.8306 - val_loss: 0.5061 - val_recall: 1.0000\n",
            "Epoch 19/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7571 - loss: 0.5417 - recall: 0.7964 - val_accuracy: 0.8387 - val_loss: 0.4965 - val_recall: 1.0000\n",
            "Epoch 20/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7195 - loss: 0.5421 - recall: 0.7182 - val_accuracy: 0.8548 - val_loss: 0.4836 - val_recall: 1.0000\n",
            "Epoch 21/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7421 - loss: 0.5427 - recall: 0.7253 - val_accuracy: 0.8710 - val_loss: 0.4754 - val_recall: 1.0000\n",
            "Epoch 22/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7440 - loss: 0.5407 - recall: 0.7198 - val_accuracy: 0.9032 - val_loss: 0.4630 - val_recall: 1.0000\n",
            "Epoch 23/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7883 - loss: 0.5250 - recall: 0.7720 - val_accuracy: 0.9032 - val_loss: 0.4563 - val_recall: 1.0000\n",
            "Epoch 24/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7798 - loss: 0.5262 - recall: 0.7989 - val_accuracy: 0.8790 - val_loss: 0.4538 - val_recall: 1.0000\n",
            "Epoch 25/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7911 - loss: 0.4841 - recall: 0.7792 - val_accuracy: 0.8710 - val_loss: 0.4484 - val_recall: 1.0000\n",
            "Epoch 26/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7628 - loss: 0.5088 - recall: 0.8125 - val_accuracy: 0.9113 - val_loss: 0.4375 - val_recall: 1.0000\n",
            "Epoch 27/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7837 - loss: 0.5116 - recall: 0.7832 - val_accuracy: 0.9194 - val_loss: 0.4288 - val_recall: 1.0000\n",
            "Epoch 28/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7960 - loss: 0.4792 - recall: 0.8301 - val_accuracy: 0.9194 - val_loss: 0.4198 - val_recall: 1.0000\n",
            "Epoch 29/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8269 - loss: 0.4628 - recall: 0.8090 - val_accuracy: 0.9194 - val_loss: 0.4158 - val_recall: 1.0000\n",
            "Epoch 30/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.7844 - loss: 0.4767 - recall: 0.8142 - val_accuracy: 0.9194 - val_loss: 0.4118 - val_recall: 1.0000\n",
            "Epoch 31/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.7698 - loss: 0.4749 - recall: 0.7659 - val_accuracy: 0.9113 - val_loss: 0.4100 - val_recall: 1.0000\n",
            "Epoch 32/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8120 - loss: 0.4656 - recall: 0.8716 - val_accuracy: 0.9194 - val_loss: 0.3963 - val_recall: 1.0000\n",
            "Epoch 33/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8151 - loss: 0.4683 - recall: 0.8198 - val_accuracy: 0.9194 - val_loss: 0.3911 - val_recall: 1.0000\n",
            "Epoch 34/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8448 - loss: 0.4444 - recall: 0.8439 - val_accuracy: 0.9194 - val_loss: 0.3858 - val_recall: 1.0000\n",
            "Epoch 35/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.7671 - loss: 0.5006 - recall: 0.7375 - val_accuracy: 0.9194 - val_loss: 0.3819 - val_recall: 1.0000\n",
            "Epoch 36/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8453 - loss: 0.4476 - recall: 0.8675 - val_accuracy: 0.9194 - val_loss: 0.3784 - val_recall: 1.0000\n",
            "Epoch 37/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8338 - loss: 0.4240 - recall: 0.8721 - val_accuracy: 0.9274 - val_loss: 0.3695 - val_recall: 1.0000\n",
            "Epoch 38/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8226 - loss: 0.4494 - recall: 0.8553 - val_accuracy: 0.9355 - val_loss: 0.3625 - val_recall: 1.0000\n",
            "Epoch 39/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.7943 - loss: 0.4482 - recall: 0.8118 - val_accuracy: 0.9435 - val_loss: 0.3571 - val_recall: 1.0000\n",
            "Epoch 40/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8292 - loss: 0.4160 - recall: 0.8305 - val_accuracy: 0.9597 - val_loss: 0.3512 - val_recall: 1.0000\n",
            "Epoch 41/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8382 - loss: 0.4352 - recall: 0.8108 - val_accuracy: 0.9435 - val_loss: 0.3493 - val_recall: 1.0000\n",
            "Epoch 42/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8697 - loss: 0.4008 - recall: 0.8571 - val_accuracy: 0.9597 - val_loss: 0.3434 - val_recall: 1.0000\n",
            "Epoch 43/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8402 - loss: 0.4245 - recall: 0.8770 - val_accuracy: 0.9597 - val_loss: 0.3376 - val_recall: 1.0000\n",
            "Epoch 44/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8063 - loss: 0.4376 - recall: 0.8011 - val_accuracy: 0.9758 - val_loss: 0.3315 - val_recall: 1.0000\n",
            "Epoch 45/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8455 - loss: 0.4281 - recall: 0.8360 - val_accuracy: 0.9597 - val_loss: 0.3303 - val_recall: 1.0000\n",
            "Epoch 46/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8547 - loss: 0.4028 - recall: 0.8733 - val_accuracy: 0.9597 - val_loss: 0.3282 - val_recall: 1.0000\n",
            "Epoch 47/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8736 - loss: 0.3747 - recall: 0.8713 - val_accuracy: 0.9597 - val_loss: 0.3216 - val_recall: 1.0000\n",
            "Epoch 48/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8746 - loss: 0.3949 - recall: 0.8612 - val_accuracy: 0.9758 - val_loss: 0.3163 - val_recall: 1.0000\n",
            "Epoch 49/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8388 - loss: 0.4061 - recall: 0.8186 - val_accuracy: 0.9758 - val_loss: 0.3106 - val_recall: 1.0000\n",
            "Epoch 50/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8460 - loss: 0.3898 - recall: 0.8248 - val_accuracy: 0.9758 - val_loss: 0.3075 - val_recall: 1.0000\n",
            "Epoch 51/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8570 - loss: 0.4020 - recall: 0.8441 - val_accuracy: 0.9758 - val_loss: 0.3049 - val_recall: 1.0000\n",
            "Epoch 52/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8769 - loss: 0.3832 - recall: 0.8404 - val_accuracy: 0.9758 - val_loss: 0.3009 - val_recall: 1.0000\n",
            "Epoch 53/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8679 - loss: 0.3845 - recall: 0.8737 - val_accuracy: 0.9758 - val_loss: 0.2966 - val_recall: 1.0000\n",
            "Epoch 54/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9076 - loss: 0.3619 - recall: 0.9198 - val_accuracy: 0.9758 - val_loss: 0.2947 - val_recall: 1.0000\n",
            "Epoch 55/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8929 - loss: 0.3681 - recall: 0.8901 - val_accuracy: 0.9758 - val_loss: 0.2904 - val_recall: 1.0000\n",
            "Epoch 56/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8716 - loss: 0.3818 - recall: 0.8954 - val_accuracy: 0.9758 - val_loss: 0.2850 - val_recall: 1.0000\n",
            "Epoch 57/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8992 - loss: 0.3637 - recall: 0.9086 - val_accuracy: 0.9758 - val_loss: 0.2818 - val_recall: 1.0000\n",
            "Epoch 58/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8938 - loss: 0.3371 - recall: 0.8938 - val_accuracy: 0.9758 - val_loss: 0.2790 - val_recall: 1.0000\n",
            "Epoch 59/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9060 - loss: 0.3284 - recall: 0.9121 - val_accuracy: 0.9758 - val_loss: 0.2755 - val_recall: 1.0000\n",
            "Epoch 60/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8977 - loss: 0.3581 - recall: 0.8896 - val_accuracy: 0.9758 - val_loss: 0.2726 - val_recall: 1.0000\n",
            "Epoch 61/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8654 - loss: 0.3520 - recall: 0.8461 - val_accuracy: 0.9758 - val_loss: 0.2706 - val_recall: 1.0000\n",
            "Epoch 62/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8821 - loss: 0.3377 - recall: 0.8486 - val_accuracy: 0.9758 - val_loss: 0.2688 - val_recall: 1.0000\n",
            "Epoch 63/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8964 - loss: 0.3295 - recall: 0.9268 - val_accuracy: 0.9839 - val_loss: 0.2623 - val_recall: 1.0000\n",
            "Epoch 64/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9021 - loss: 0.3295 - recall: 0.9510 - val_accuracy: 0.9839 - val_loss: 0.2592 - val_recall: 1.0000\n",
            "Epoch 65/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9101 - loss: 0.3056 - recall: 0.9150 - val_accuracy: 0.9839 - val_loss: 0.2579 - val_recall: 1.0000\n",
            "Epoch 66/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8913 - loss: 0.3294 - recall: 0.8807 - val_accuracy: 0.9839 - val_loss: 0.2559 - val_recall: 1.0000\n",
            "Epoch 67/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9173 - loss: 0.3244 - recall: 0.9267 - val_accuracy: 0.9839 - val_loss: 0.2525 - val_recall: 1.0000\n",
            "Epoch 68/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9030 - loss: 0.3163 - recall: 0.8981 - val_accuracy: 0.9839 - val_loss: 0.2469 - val_recall: 1.0000\n",
            "Epoch 69/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9171 - loss: 0.3104 - recall: 0.9078 - val_accuracy: 0.9839 - val_loss: 0.2455 - val_recall: 1.0000\n",
            "Epoch 70/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8844 - loss: 0.3318 - recall: 0.8926 - val_accuracy: 0.9839 - val_loss: 0.2428 - val_recall: 1.0000\n",
            "Epoch 71/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9155 - loss: 0.2921 - recall: 0.9214 - val_accuracy: 0.9839 - val_loss: 0.2397 - val_recall: 1.0000\n",
            "Epoch 72/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8842 - loss: 0.3217 - recall: 0.8868 - val_accuracy: 0.9839 - val_loss: 0.2385 - val_recall: 1.0000\n",
            "Epoch 73/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9005 - loss: 0.3098 - recall: 0.8948 - val_accuracy: 0.9839 - val_loss: 0.2350 - val_recall: 1.0000\n",
            "Epoch 74/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9247 - loss: 0.3182 - recall: 0.9270 - val_accuracy: 0.9839 - val_loss: 0.2321 - val_recall: 1.0000\n",
            "Epoch 75/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9063 - loss: 0.3128 - recall: 0.9021 - val_accuracy: 0.9839 - val_loss: 0.2297 - val_recall: 1.0000\n",
            "Epoch 76/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9337 - loss: 0.3070 - recall: 0.9212 - val_accuracy: 0.9839 - val_loss: 0.2285 - val_recall: 1.0000\n",
            "Epoch 77/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9135 - loss: 0.3032 - recall: 0.9422 - val_accuracy: 0.9839 - val_loss: 0.2250 - val_recall: 1.0000\n",
            "Epoch 78/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9147 - loss: 0.3029 - recall: 0.9031 - val_accuracy: 0.9839 - val_loss: 0.2232 - val_recall: 1.0000\n",
            "Epoch 79/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9193 - loss: 0.2884 - recall: 0.9112 - val_accuracy: 0.9839 - val_loss: 0.2227 - val_recall: 1.0000\n",
            "Epoch 80/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9555 - loss: 0.2683 - recall: 0.9593 - val_accuracy: 0.9839 - val_loss: 0.2209 - val_recall: 1.0000\n",
            "Epoch 81/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9130 - loss: 0.2849 - recall: 0.9336 - val_accuracy: 0.9839 - val_loss: 0.2186 - val_recall: 1.0000\n",
            "Epoch 82/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9262 - loss: 0.2760 - recall: 0.9224 - val_accuracy: 0.9839 - val_loss: 0.2165 - val_recall: 1.0000\n",
            "Epoch 83/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9534 - loss: 0.2565 - recall: 0.9814 - val_accuracy: 0.9839 - val_loss: 0.2141 - val_recall: 1.0000\n",
            "Epoch 84/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9190 - loss: 0.2863 - recall: 0.9009 - val_accuracy: 0.9839 - val_loss: 0.2102 - val_recall: 1.0000\n",
            "Epoch 85/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.8951 - loss: 0.3085 - recall: 0.8992 - val_accuracy: 0.9839 - val_loss: 0.2102 - val_recall: 1.0000\n",
            "Epoch 86/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9373 - loss: 0.2444 - recall: 0.9454 - val_accuracy: 0.9839 - val_loss: 0.2070 - val_recall: 1.0000\n",
            "Epoch 87/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9311 - loss: 0.2707 - recall: 0.9410 - val_accuracy: 0.9839 - val_loss: 0.2046 - val_recall: 1.0000\n",
            "Epoch 88/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9233 - loss: 0.3008 - recall: 0.9137 - val_accuracy: 0.9839 - val_loss: 0.2034 - val_recall: 1.0000\n",
            "Epoch 89/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9244 - loss: 0.2791 - recall: 0.9300 - val_accuracy: 0.9839 - val_loss: 0.2008 - val_recall: 1.0000\n",
            "Epoch 90/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9378 - loss: 0.2638 - recall: 0.9369 - val_accuracy: 0.9919 - val_loss: 0.1980 - val_recall: 1.0000\n",
            "Epoch 91/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9349 - loss: 0.2523 - recall: 0.9201 - val_accuracy: 0.9839 - val_loss: 0.1973 - val_recall: 1.0000\n",
            "Epoch 92/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9344 - loss: 0.2537 - recall: 0.9605 - val_accuracy: 0.9839 - val_loss: 0.1949 - val_recall: 1.0000\n",
            "Epoch 93/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9446 - loss: 0.2355 - recall: 0.9338 - val_accuracy: 0.9839 - val_loss: 0.1944 - val_recall: 1.0000\n",
            "Epoch 94/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9394 - loss: 0.2563 - recall: 0.9363 - val_accuracy: 0.9839 - val_loss: 0.1917 - val_recall: 1.0000\n",
            "Epoch 95/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9231 - loss: 0.2472 - recall: 0.9127 - val_accuracy: 0.9839 - val_loss: 0.1900 - val_recall: 1.0000\n",
            "Epoch 96/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9101 - loss: 0.2817 - recall: 0.9078 - val_accuracy: 0.9839 - val_loss: 0.1899 - val_recall: 1.0000\n",
            "Epoch 97/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9446 - loss: 0.2328 - recall: 0.9364 - val_accuracy: 0.9839 - val_loss: 0.1874 - val_recall: 1.0000\n",
            "Epoch 98/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9438 - loss: 0.2465 - recall: 0.9348 - val_accuracy: 0.9839 - val_loss: 0.1875 - val_recall: 1.0000\n",
            "Epoch 99/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9403 - loss: 0.2450 - recall: 0.9566 - val_accuracy: 0.9839 - val_loss: 0.1841 - val_recall: 1.0000\n",
            "Epoch 100/100\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9562 - loss: 0.2275 - recall: 0.9643 - val_accuracy: 0.9919 - val_loss: 0.1819 - val_recall: 1.0000\n",
            "Saved artifact at '/tmp/tmpflsba9z0'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_535')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  138782340955792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340955216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340954256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340954640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340954448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340941392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340955024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340954832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340955600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340953296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340953872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340955984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340949072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340953104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340952528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340953680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340955408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340952720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340951952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340952912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340954064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340952144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340953488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340950992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340951568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340951376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340951184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340948880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340950032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340950608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340950416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340950224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340952336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340948496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340949648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340949456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340949264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340951760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340947728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340948304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340948112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340947920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340950800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340946768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340947344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340947152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340946960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340949840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340945808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340946384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340946192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340946000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340948688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340944848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340945424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340945232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340945040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340947536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340943888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340944464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340946576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340943120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340943696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340942736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340944272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340945616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340942928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340942160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340943504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340944656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340942544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340941776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340941968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138785008205904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340941200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340944080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340941584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340943312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340942352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340448528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340450064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340449296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340450448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340448912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340449488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340450256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340451024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340448336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340449680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340450832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340449104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340451984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340451408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340451600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340451792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340449872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340452944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340452368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340450640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340453712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340453136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340454096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340452560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340451216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340448720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340454672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340453328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340452176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340454480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340452752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340455632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340455056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340455248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340455440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340453520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340456592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340456016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340454288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340457360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340456784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340457744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340456208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340454864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340457552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340458320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340456976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340455824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340458128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340456400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340459280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340458704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340458896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340459088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340457168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340460240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340459664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340457936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340461008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340460432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340461392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340459856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340458512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340461200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340461968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340460624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340459472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340461776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340460048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340462928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340462352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340462544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340462736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340460816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340463888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340463312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340464464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340463504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340464272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340463696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340464080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340462160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340461584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782340463120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220484880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220485264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220485072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220485456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220486608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220486032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220486224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220486416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220484688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220487568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220486992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220485648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220488336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220487760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220488720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220487184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220485840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220488528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220489296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220487952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220486800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220489104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220487376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220490256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220489680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220489872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220490064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220488144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220491216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220490640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220488912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220491984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220491408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220492368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220490832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220489488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220492176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220492944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220491600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220490448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220492752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220491024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220493904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220494288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  138782220493712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "MobileNet Saved: /content/drive/MyDrive/tex_cars/mobilenet/v2/hood_classifier.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing import image\n",
        "# from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# # 1. Load TFLite Model\n",
        "# interpreter = tf.lite.Interpreter(model_path=mn_path)\n",
        "# interpreter.allocate_tensors()\n",
        "\n",
        "# input_details = interpreter.get_input_details()\n",
        "# output_details = interpreter.get_output_details()\n",
        "# input_index = input_details[0]['index']\n",
        "# output_index = output_details[0]['index']\n",
        "\n",
        "# print(\"Model loaded. Starting inference...\")\n",
        "\n",
        "# predictions = []\n",
        "# actuals = []\n",
        "\n",
        "# # 2. Iterate through Validation Data\n",
        "# # Assuming val_df has columns 'filepath' and 'is_hood_open_str' (or numeric label)\n",
        "# for index, row in val_df.iterrows():\n",
        "#     img_path = row['filepath']\n",
        "\n",
        "#     # Load and Preprocess (Must match training exactly)\n",
        "#     try:\n",
        "#         img = image.load_img(img_path, target_size=(224, 224))\n",
        "#         x = image.img_to_array(img)\n",
        "#         x = preprocess_input(x) # MobileNetV3 specific preprocessing\n",
        "#         x = np.expand_dims(x, axis=0) # Add batch dimension\n",
        "\n",
        "#         # Run Inference\n",
        "#         interpreter.set_tensor(input_index, x)\n",
        "#         interpreter.invoke()\n",
        "#         output_data = interpreter.get_tensor(output_index)\n",
        "\n",
        "#         # Binary thresholding (Sigmoid output)\n",
        "#         pred_label = 1 if output_data[0][0] > 0.5 else 0\n",
        "#         predictions.append(pred_label)\n",
        "\n",
        "#         # Get actual label (handle string or int)\n",
        "#         actual = row['is_hood_open_str']\n",
        "#         actual_label = 1 if str(actual) == '1' or str(actual).lower() == 'open' else 0\n",
        "#         actuals.append(actual_label)\n",
        "\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error processing {img_path}: {e}\")\n",
        "\n",
        "# # 3. Metrics\n",
        "# print(f\"\\nAccuracy: {accuracy_score(actuals, predictions):.4f}\")\n",
        "# print(\"\\nClassification Report:\\n\")\n",
        "# print(classification_report(actuals, predictions, target_names=['Closed', 'Open']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRAQ_YpZ2D0q",
        "outputId": "ab64b316-e3d6-48aa-c132-b89c4ebc1fd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/tensorflow/lite/python/interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
            "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
            "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
            "    for details.\n",
            "    \n",
            "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded. Starting inference...\n",
            "\n",
            "Accuracy: 0.9677\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Closed       1.00      0.94      0.97        62\n",
            "        Open       0.94      1.00      0.97        62\n",
            "\n",
            "    accuracy                           0.97       124\n",
            "   macro avg       0.97      0.97      0.97       124\n",
            "weighted avg       0.97      0.97      0.97       124\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6ZpI6QOZxdGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YOLO"
      ],
      "metadata": {
        "id": "epwNXYn52ihq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import glob\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "print(\"\\n--- Training YOLOv11 with Live Precision/Recall ---\")\n",
        "\n",
        "# 1. Install Ultralytics\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "except ImportError:\n",
        "    subprocess.check_call([\"pip\", \"install\", \"ultralytics\"])\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "# 2. Prepare Directory Structure\n",
        "yolo_data_dir = '/content/yolo_dataset'\n",
        "val_path = os.path.join(yolo_data_dir, 'val') # Define globally for callback\n",
        "\n",
        "if os.path.exists(yolo_data_dir):\n",
        "    shutil.rmtree(yolo_data_dir)\n",
        "\n",
        "def get_class_folder(label):\n",
        "    s_label = str(label).lower()\n",
        "    if s_label == '1' or s_label == 'open':\n",
        "        return 'Open'\n",
        "    return 'Closed'\n",
        "\n",
        "# Create train and val directories\n",
        "train_dir_yolo = os.path.join(yolo_data_dir, 'train')\n",
        "val_dir_yolo = os.path.join(yolo_data_dir, 'val')\n",
        "os.makedirs(train_dir_yolo, exist_ok=True)\n",
        "os.makedirs(val_dir_yolo, exist_ok=True)\n",
        "\n",
        "for subset_name, df_part in [('train', train_df), ('val', val_df)]:\n",
        "    for _, row in df_part.iterrows():\n",
        "        folder_name = get_class_folder(row['is_hood_open_str'])\n",
        "        # Use the specific subset directory (train_dir_yolo or val_dir_yolo)\n",
        "        dest_dir = os.path.join(yolo_data_dir, subset_name, folder_name)\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "        shutil.copy(row['filepath'], os.path.join(dest_dir, os.path.basename(row['filepath'])))\n",
        "\n",
        "\n",
        "# 3. Initialize Model\n",
        "model_yolo = YOLO(\"yolo11m-cls.pt\")\n",
        "\n",
        "# 4. Define Custom Callback\n",
        "def on_train_epoch_end(trainer):\n",
        "    \"\"\"\n",
        "    Custom callback to calculate P/R at the end of every epoch.\n",
        "    \"\"\"\n",
        "    print(f\"\\n[Epoch {trainer.epoch + 1}] Calculating Precision/Recall...\")\n",
        "\n",
        "    # FIX: Do NOT use trainer.model directly as it breaks the gradient graph.\n",
        "    # Instead, load the latest saved weights from disk into a fresh instance.\n",
        "    weight_path = os.path.join(trainer.save_dir, 'weights', 'last.pt')\n",
        "\n",
        "    if not os.path.exists(weight_path):\n",
        "        print(\"   >> No saved weights found yet. Skipping metrics.\")\n",
        "        return\n",
        "\n",
        "    # Load temp model for safe inference\n",
        "    temp_model = YOLO(weight_path)\n",
        "\n",
        "    # Gather validation images\n",
        "    val_images = glob.glob(os.path.join(val_path, '**', '*'), recursive=True)\n",
        "    val_images = [f for f in val_images if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp'))]\n",
        "\n",
        "    if not val_images:\n",
        "        print(f\"⚠️ Warning: No images found in {val_path}. Skipping metrics.\")\n",
        "        return\n",
        "\n",
        "    # Run inference\n",
        "    results = temp_model.predict(source=val_images, stream=True, verbose=False)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for r in results:\n",
        "        # Extract true label from folder structure\n",
        "        folder = os.path.basename(os.path.dirname(r.path))\n",
        "        true_label = 1 if folder.lower() == 'open' else 0\n",
        "        y_true.append(true_label)\n",
        "\n",
        "        # Extract predicted label\n",
        "        pred_class_name = temp_model.names[r.probs.top1]\n",
        "        pred_label = 1 if pred_class_name.lower() == 'open' else 0\n",
        "        y_pred.append(pred_label)\n",
        "\n",
        "    # Calculate Metrics\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"   >> Precision (Open): {precision:.4f}\")\n",
        "    print(f\"   >> Recall    (Open): {recall:.4f}\")\n",
        "    print(f\"   >> F1-Score  (Open): {f1:.4f}\\n\")\n",
        "\n",
        "    # Clean up to save memory\n",
        "    del temp_model\n",
        "\n",
        "# 5. Register Callback and Train\n",
        "model_yolo.add_callback(\"on_train_epoch_end\", on_train_epoch_end)\n",
        "\n",
        "# Define run parameters\n",
        "project_path = '/content/yolo_runs'\n",
        "run_name = 'hood_run'\n",
        "\n",
        "model_yolo.train(\n",
        "    data=yolo_data_dir,\n",
        "    epochs=3,\n",
        "    imgsz=224,\n",
        "    project='/content/yolo_runs',\n",
        "    name='hood_run',\n",
        "    # Augmentations\n",
        "    degrees=180,\n",
        "    flipud=0.5,\n",
        "    fliplr=0.5,\n",
        "    mosaic=1.0,\n",
        "    scale=0.5,\n",
        "    erasing=0.4,\n",
        "    mixup=0.1\n",
        ")\n",
        "\n",
        "# 6. ROBUST EXPORT & SAVE\n",
        "print(\"\\n--- Locating Latest Run and Exporting ---\")\n",
        "\n",
        "# 1. Find the latest run folder (e.g., hood_run, hood_run2, hood_run3)\n",
        "# We search for all folders starting with 'hood_run' in the project directory\n",
        "possible_runs = glob.glob(os.path.join(project_path, f\"{run_name}*\"))\n",
        "\n",
        "if possible_runs:\n",
        "    # Select the folder with the most recent modification time\n",
        "    latest_run_dir = max(possible_runs, key=os.path.getmtime)\n",
        "    print(f\"📍 Detected latest run: {latest_run_dir}\")\n",
        "\n",
        "    # Define path to best.pt in this specific run\n",
        "    best_weight_path = os.path.join(latest_run_dir, 'weights', 'best.pt')\n",
        "else:\n",
        "    print(f\"❌ Error: No folders found starting with {run_name}\")\n",
        "    exit()\n",
        "\n",
        "# 2. Proceed with Export if weights exist\n",
        "if os.path.exists(best_weight_path):\n",
        "    print(f\"Found best model weights: {best_weight_path}\")\n",
        "\n",
        "    # Save .pt to Drive\n",
        "    pt_dest = os.path.join(yolo_save_dir, 'yolo_hood_best.pt')\n",
        "    shutil.copy(best_weight_path, pt_dest)\n",
        "    print(f\"✅ Saved .pt weights to: {pt_dest}\")\n",
        "\n",
        "    # Export to TFLite\n",
        "    print(\"Exporting to TFLite...\")\n",
        "    best_model = YOLO(best_weight_path)\n",
        "    best_model.export(format='tflite')\n",
        "\n",
        "    # Find the generated TFLite file (it might be in the same folder or a subdir)\n",
        "    found_tflites = glob.glob(os.path.join(latest_run_dir, '**', '*.tflite'), recursive=True)\n",
        "\n",
        "    if found_tflites:\n",
        "        # Get the newest TFLite file just in case\n",
        "        source_tflite = max(found_tflites, key=os.path.getmtime)\n",
        "\n",
        "        tflite_dest = os.path.join(yolo_save_dir, 'yolo_hood_best.tflite')\n",
        "        shutil.copy(source_tflite, tflite_dest)\n",
        "        print(f\"✅ Saved TFLite model to: {tflite_dest}\")\n",
        "    else:\n",
        "        print(\"❌ Error: TFLite export failed or file not found.\")\n",
        "else:\n",
        "    print(f\"❌ Error: 'best.pt' missing in {latest_run_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8qdLXZ2nLsE",
        "outputId": "1812e5ad-8fc2-47c3-f8ce-19aff43cf9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training YOLOv11 with Live Precision/Recall ---\n",
            "Ultralytics 8.3.232 🚀 Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/yolo_dataset, degrees=180, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolo11m-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=hood_run10, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/yolo_runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/yolo_runs/hood_run10, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/yolo_dataset/train... found 488 images in 2 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/yolo_dataset/val... found 124 images in 2 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
            "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
            "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
            "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
            "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
            "  9                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
            " 10                  -1  1    660482  ultralytics.nn.modules.head.Classify         [512, 2]                      \n",
            "YOLO11m-cls summary: 106 layers, 10,355,778 parameters, 10,355,778 gradients, 39.6 GFLOPs\n",
            "Transferred 294/296 items from pretrained weights\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1527.3±384.1 MB/s, size: 117.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolo_dataset/train... 488 images, 0 corrupt: 100% ━━━━━━━━━━━━ 488/488 346.5it/s 1.4s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolo_dataset/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1317.2±674.9 MB/s, size: 121.0 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolo_dataset/val... 124 images, 0 corrupt: 100% ━━━━━━━━━━━━ 124/124 356.0it/s 0.3s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolo_dataset/val.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 49 weight(decay=0.0), 50 weight(decay=0.0005), 50 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/yolo_runs/hood_run10\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K        1/3         0G     0.3401          8        224: 100% ━━━━━━━━━━━━ 31/31 8.7s/it 4:28\n",
            "\n",
            "[Epoch 1] Calculating Precision/Recall...\n",
            "   >> No saved weights found yet. Skipping metrics.\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 4/4 5.9s/it 23.5s\n",
            "                   all          1          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K        2/3         0G    0.04457         16        224: 26% ━━━───────── 8/31 8.8s/it 1:17<3:23"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Exporting to OpenVINO...\")\n",
        "best_model = YOLO(best_weight_path)\n",
        "\n",
        "# Export args:\n",
        "# half=True (FP16 quantization, faster on modern CPUs/GPUs, slight acc loss)\n",
        "# int8=True (INT8 quantization, much faster, requires valid data for calibration)\n",
        "openvino_dir = best_model.export(format='openvino', half=True)\n",
        "\n",
        "print(f\"Exported to: {openvino_dir}\")\n",
        "\n",
        "# --- Zip and Save to Drive ---\n",
        "if os.path.exists(openvino_dir):\n",
        "    # Define zip name\n",
        "    zip_name = \"yolo_hood_openvino\"\n",
        "    zip_path = os.path.join(yolo_save_dir, zip_name)\n",
        "\n",
        "    # Create zip file (shutil.make_archive adds .zip automatically)\n",
        "    shutil.make_archive(zip_path, 'zip', openvino_dir)\n",
        "\n",
        "    print(f\"✅ OpenVINO model zipped and saved to: {zip_path}.zip\")\n",
        "else:\n",
        "    print(\"❌ Error: OpenVINO export directory not found.\")"
      ],
      "metadata": {
        "id": "bcOum82x7Gcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}